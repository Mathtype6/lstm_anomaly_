multi_step_lstm_config = {  'batch_size': 370,
                            'n_epochs': 100,
                            'dropout': 0.1,
                            'look_back': 3,
                            'look_ahead': 1,
                            #'layers':{'input': 1, 'hidden1':20, 'hidden2':5,  'output': 1},
                            #'layers':{'input': 1, 'hidden1': 200, 'hidden2': 80, 'hidden3': 40, 'hidden4': 10,'output': 1},
                            'layers': {'input': 1, 'hidden1': 60, 'hidden2': 60, 'output': 1},
                            'loss': 'mse',
                            #'optimizer': 'adam',
                            'train_test_ratio' : 0.7,
                            'shuffle': False,
                            'validation': True,
                            'learning_rate': .1,
                            'patience': 5,
                           }
#

2017-05-25 19:47:03,117. 
2017-05-25 19:48:07,438. ----------------------------------------------------
2017-05-25 19:48:07,438. Run id ECG_may25
2017-05-25 19:48:07,438.  HYPERPRAMRAMS : {'layers': {'input': 1, 'hidden1': 60, 'output': 1, 'hidden2': 60}, 'loss': 'mse', 'shuffle': False, 'learning_rate': 0.1, 'look_ahead': 1, 'batch_size': 367, 'epochs': 100, 'patience': 5, 'experiment_id': 'ECG_may25', 'data_folder': 'resources/data/discords/ECG/', 'validation': True, 'look_back': 3, 'dropout': 0.1}
2017-05-25 19:48:07,477. StatefulMultiStepLSTM LSTM Model Info: {'layers': {'input': 1, 'hidden1': 60, 'output': 1, 'hidden2': 60}, 'loss': 'mse', 'learning_rate': 0.1, 'look_ahead': 1, 'batch_size': 367, 'self': <models.lstm.StatefulMultiStepLSTM object at 0x7f0806529ad0>, 'look_back': 3, 'dropout': 0.1}
2017-05-25 19:48:08,480. Compilation Time : 0.0295031070709
2017-05-25 19:48:08,532. Training...
2017-05-25 19:49:48,182. Training duration (s) : 99.6501569748
2017-05-25 19:49:48,184. Training Loss per epoch: [0.55872116284444928, 0.10457791516091675, 0.067016234155744314, 0.057828545104712248, 0.05204383737873286, 0.047162168077193201, 0.045413329033181071, 0.042435569106601179, 0.041454130841884762, 0.039705869741737843, 0.038634089403785765, 0.038246951124165207, 0.036722338991239667, 0.03635885362746194, 0.036253392754588276, 0.035421852837316692, 0.034215209365356714, 0.034161816525738686, 0.034012557996902615, 0.033093158039264381, 0.032724166405387223, 0.032513551705051214, 0.032073216745629907, 0.031986184534616768, 0.031767797074280679, 0.031258077244274318, 0.030985285062342882, 0.031110135081689805, 0.03001427004346624, 0.030868596280924976, 0.029392593598458916, 0.029568896221462637, 0.029714780044741929, 0.030013445124495775, 0.029147389810532331, 0.029991241521202028, 0.029837700538337231, 0.02924654254456982, 0.028635106980800629, 0.028194105776492506, 0.02795435837469995, 0.027546933852136135, 0.028389697021339089, 0.028067529608961195, 0.028210861899424344, 0.027810225612483919, 0.027833996340632439, 0.027227633108850569, 0.027405751403421164, 0.028011017944663763, 0.028035232564434409, 0.027443925791885704, 0.027250932587776333, 0.02690967993112281, 0.026667787111364305, 0.026397937559522688, 0.027005757379811257, 0.026366875041276217, 0.026358969509601593, 0.026554071460850537, 0.02654268394690007, 0.026601061399560422, 0.026105533295776695, 0.026590682100504637, 0.026031712652184069, 0.026285910455044359, 0.026163112372159958, 0.026235363155137748, 0.025775794347282499, 0.026105078053660691, 0.02618950919713825, 0.02627952751936391, 0.025242962641641498, 0.025675606040749699, 0.02553815086139366, 0.025428920343983918, 0.026057191542349756, 0.025276521104387939, 0.025727497879415751, 0.025833005551248789, 0.025545764539856464, 0.024913961242418736, 0.025371566589456052, 0.025458113930653781, 0.025043471250683069, 0.025050090800505131, 0.025874884566292167, 0.024629791674669832, 0.024817176745273173, 0.024441996589303017, 0.024793352989945561, 0.024595500726718456, 0.025022312125656754, 0.025320887623820454, 0.025987686996813864, 0.025039333151653409]
2017-05-25 19:49:48,184. Validation  Loss per epoch: [0.12070849041144054, 0.083795296649138137, 0.053454022854566574, 0.043931937466065087, 0.037748670826355614, 0.033440424129366875, 0.030588350569208462, 0.028598691026369732, 0.027316326275467873, 0.025970845172802608, 0.025327228630582493, 0.024309902762373287, 0.023588409647345543, 0.023030957207083702, 0.022424312308430672, 0.022149346147974331, 0.02143183598915736, 0.021313481032848358, 0.020698507626851399, 0.020431414867440861, 0.020510056366523106, 0.020070519919196766, 0.019857022290428478, 0.01970260279874007, 0.019518374775846798, 0.019474354262153309, 0.019328666230042774, 0.01925222637752692, 0.018761839096744854, 0.018594160055120785, 0.018431562619904678, 0.018616259098052979, 0.018090091335276764, 0.01817856418589751, 0.01806986176719268, 0.017945011146366596, 0.018033495172858238, 0.017833856555322807, 0.017762108705937862, 0.01771169373144706, 0.017852255764106911, 0.017447597347199917, 0.017306759332617123, 0.01722453876088063, 0.017295911597708862, 0.017181867423156898, 0.017044364474713802, 0.016988129355013371, 0.016934295805792015, 0.017006604621807735, 0.016864374590416748, 0.01681035788108905, 0.016799413599073887, 0.016716502917309601, 0.016655058289567631, 0.016697670022646587, 0.016558076255023479, 0.016679566353559494, 0.016514216239253681, 0.01651094698657592, 0.016514585042993229, 0.016616239833335083, 0.016519256867468357, 0.016349685378372669, 0.016300332732498646, 0.016356644220650196, 0.016305157914757729, 0.016281802207231522, 0.016113582688073318, 0.016349435473481815, 0.016200797011454899, 0.016028850339353085, 0.016110950460036595, 0.016106128382186096, 0.015999687214692433, 0.01592606430252393, 0.015891872346401215, 0.015975183186431725, 0.016035335448880989, 0.015858178958296776, 0.015826476116975147, 0.015916249714791775, 0.015710015781223774, 0.015751366813977558, 0.015592077126105627, 0.015643136265377205, 0.015637398386994999, 0.015676719757417839, 0.015604765154421329, 0.01545413862913847, 0.015701294255753357, 0.015656158017615478, 0.015522774308919907, 0.015540394311149916, 0.015636792095998924, 0.015482779902716478]
2017-05-25 19:49:50,952. Validation2 Loss 0.102771230896
2017-05-25 19:49:51,094. Test Loss 0.102429188943
2017-05-25 19:49:51,642. Train RMSE on 1 timestep prediction 0.020152
2017-05-25 19:49:51,643.  Train RMSE Naive One Timestep Shift 0.020917
2017-05-25 19:49:55,995. Validation1 RMSE on 1 timestep prediction 0.019146
2017-05-25 19:49:55,995.  Validation1 RMSE Naive One Timestep Shift 0.020202
2017-05-25 19:49:58,076. Validation2 RMSE on 1 timestep prediction 0.049309
2017-05-25 19:49:58,076.  Validation2 RMSE Naive One Timestep Shift 0.035025
2017-05-25 19:50:17,017. Test RMSE on 1 timestep prediction 0.049226
2017-05-25 19:50:17,017.  Test RMSE Naive One Timestep Shift 0.035025
2017-05-25 19:50:23,306. -------------------------run complete----------------------------------------------


*********************************************************************************************************************************************************************
Lookahead 5


run_config = { 'Xserver' : True,
               'log_file' : 'logs/run.log',
               'experiment_id' : "ECG_may25",
               #'data_folder': 'resources/data/nab/nab_machine_temperature/',
               #'data_folder': 'resources/data/discords/space_shuttle/',
               #'data_folder': 'resources/data/discords/dutch_power/',
               'data_folder': 'resources/data/discords/ECG/',
               'save_figure': True
               }

opt_config = { 'Xserver' : True,
               'log_file' : '../logs/opt.log',
               'opt_run_id': "machine_temp_may24",
               'data_folder': '../resources/data/discords/dutch_power/',
               'save_figure': True,
               'model': 'stateful',
               'max_iter': 3,
                'initial_evals': 1
               }

multi_step_lstm_config = {  'batch_size': 370,
                            'n_epochs': 100,
                            'dropout': 0.1,
                            'look_back': 3,
                            'look_ahead': 5,
                            #'layers':{'input': 1, 'hidden1':20, 'hidden2':5,  'output': 1},
                            #'layers':{'input': 1, 'hidden1': 200, 'hidden2': 80, 'hidden3': 40, 'hidden4': 10,'output': 1},
                            'layers': {'input': 1, 'hidden1': 60, 'hidden2': 60, 'output': 1},
                            'loss': 'mse',
                            #'optimizer': 'adam',
                            'train_test_ratio' : 0.7,
                            'shuffle': False,
                            'validation': True,
                            'learning_rate': .1,
                            'patience': 5,
                           }
#

2017-05-25 20:18:40,311. Run id ECG_may25
2017-05-25 20:18:40,312.  HYPERPRAMRAMS : {'layers': {'input': 1, 'hidden1': 60, 'output': 1, 'hidden2': 60}, 'loss': 'mse', 'shuffle': False, 'learning_rate': 0.1, 'look_ahead': 5, 'batch_size': 363, 'epochs': 100, 'patience': 5, 'experiment_id': 'ECG_may25', 'data_folder': 'resources/data/discords/ECG/', 'validation': True, 'look_back': 3, 'dropout': 0.1}
2017-05-25 20:18:40,352. StatefulMultiStepLSTM LSTM Model Info: {'layers': {'input': 1, 'hidden1': 60, 'output': 1, 'hidden2': 60}, 'loss': 'mse', 'learning_rate': 0.1, 'look_ahead': 5, 'batch_size': 363, 'self': <models.lstm.StatefulMultiStepLSTM object at 0x7f08c6bfaad0>, 'look_back': 3, 'dropout': 0.1}
2017-05-25 20:18:41,321. Compilation Time : 0.0297451019287
2017-05-25 20:18:41,408. Training...
2017-05-25 20:20:32,150. Training duration (s) : 110.741152048
2017-05-25 20:20:32,150. Training Loss per epoch: [1.681851084344089, 0.24788869777694345, 0.19641943322494626, 0.18119888962246478, 0.17181257531046867, 0.16298708878457546, 0.15975677547976375, 0.15461506973952055, 0.15103128179907799, 0.14764749491587281, 0.1472747188527137, 0.14339367137290537, 0.14283517911098897, 0.13769937749020755, 0.13583468785509467, 0.13662779750302434, 0.1321789612993598, 0.13268032530322671, 0.13176460331305861, 0.13355643046088517, 0.1305908274371177, 0.12923583178780973, 0.12567584379576147, 0.12652167002670467, 0.12611427786760032, 0.12515893741510808, 0.12563479738309979, 0.1243357895873487, 0.12199711962603033, 0.1233309677336365, 0.11901260400190949, 0.11996645550243556, 0.11969380197115242, 0.12172879581339657, 0.11937546846456826, 0.11721777776256204, 0.11710855853743851, 0.11774927005171776, 0.11638905853033066, 0.11628759372979403, 0.11468136333860457, 0.11609931499697268, 0.11453231913037598, 0.11322964634746313, 0.11264469986781478, 0.1108244857750833, 0.11131957662291825, 0.11192723549902439, 0.11287318845279515, 0.11310212803073227, 0.11135042947717011, 0.11200372036546469, 0.11064658127725124, 0.11075551644898951, 0.10832354892045259, 0.10980218579061329, 0.10813198331743479, 0.10956925828941166, 0.10642905556596816, 0.11058834567666054, 0.10694219567812979, 0.10882816440425813, 0.10725958039984107, 0.10749708325602114, 0.10792173980735242, 0.10765618621371686, 0.10601341794244945, 0.10687973094172776, 0.10617560381069779, 0.10673096892423928, 0.1044313448946923, 0.10791590181179345, 0.10681095626205206, 0.10740753263235092, 0.1064254161901772, 0.10419251304119825, 0.10395475965924561, 0.10340923094190657, 0.10595294320955873, 0.10277637699618936, 0.10480691585689783, 0.10199858248233795, 0.10318024479784071, 0.10393365821801126, 0.10373162105679512, 0.10230897483415902, 0.103138581616804, 0.10452512931078672, 0.10411519557237625, 0.10311331041157246, 0.10226640338078141, 0.10122654028236866, 0.1030129874125123, 0.10228489059954882, 0.10304215038195252, 0.10130464751273394, 0.10123215150088072, 0.10114802233874798, 0.10119700338691473, 0.10003186669200659]
2017-05-25 20:20:32,150. Validation  Loss per epoch: [0.30018413563569385, 0.20086294909318289, 0.18075448771317801, 0.16436824202537537, 0.15877486765384674, 0.15527642269929251, 0.15215791761875153, 0.14801597595214844, 0.14628015955289206, 0.14373635252316794, 0.14172312120596567, 0.14060577750205994, 0.13783262670040131, 0.1358694757024447, 0.13479461769262949, 0.13352735588947931, 0.13177115221818289, 0.13065698246161142, 0.1290183812379837, 0.12673052400350571, 0.12612599631150564, 0.12457663814226787, 0.12311996767918269, 0.12122482309738795, 0.12064347912867864, 0.12005428473154704, 0.11839357763528824, 0.11648839960495631, 0.11621078352133433, 0.11544438699881236, 0.11403590937455495, 0.1129807357986768, 0.11195002247889836, 0.11122623831033707, 0.11105271925528844, 0.10987007617950439, 0.10863147675991058, 0.10847194244464238, 0.10686417420705159, 0.10637943198283513, 0.10640262812376022, 0.10482439895470937, 0.10476664453744888, 0.10368691136439641, 0.10398734857638676, 0.10328663637240727, 0.10247526069482167, 0.10183002551396687, 0.10140292346477509, 0.10054255276918411, 0.10034240782260895, 0.10014313211043675, 0.099577431877454117, 0.099612404902776078, 0.09902036190032959, 0.098566172023614243, 0.097679309546947479, 0.09764484564463298, 0.096681691706180573, 0.096642243365446731, 0.096274316310882568, 0.096471617619196579, 0.095548011362552643, 0.095530492564042405, 0.095335955421129867, 0.095063361028830215, 0.095153945187727615, 0.094620245198408767, 0.0939405436317126, 0.093528566261132554, 0.094118632376194, 0.093746677041053772, 0.093818070987860366, 0.093401342630386353, 0.092878920336564377, 0.093227701882521316, 0.093187324702739716, 0.092584654688835144, 0.092521240313847855, 0.09223128606875737, 0.092145994305610657, 0.091640911996364594, 0.091501509149869278, 0.091284915804862976, 0.091127162178357438, 0.090647963186105088, 0.090260999898115798, 0.090523339807987213, 0.090656193594137832, 0.09033975501855214, 0.090266473591327667, 0.090354104836781815, 0.08979024986426036, 0.089935059348742172, 0.089866943657398224, 0.089401779075463608, 0.089614274601141616, 0.089659643669923142, 0.089190281927585602, 0.089018200834592179]
2017-05-25 20:20:42,063. Validation2 Loss 0.337520707112
2017-05-25 20:20:42,227. Test Loss 0.339369262067
2017-05-25 20:20:42,825. Train RMSE on 1 timestep prediction 0.027592
2017-05-25 20:20:42,827. Train RMSE on 2 timestep prediction 0.036503
2017-05-25 20:20:42,827. Train RMSE on 3 timestep prediction 0.045102
2017-05-25 20:20:42,827. Train RMSE on 4 timestep prediction 0.052564
2017-05-25 20:20:42,827. Train RMSE on 5 timestep prediction 0.060044
2017-05-25 20:20:42,82  8.  Train RMSE Naive One Timestep Shift 0.020872
2017-05-25 20:21:57,913. Validation1 RMSE on 1 timestep prediction 0.031064
2017-05-25 20:21:57,913. Validation1 RMSE on 2 timestep prediction 0.037720
2017-05-25 20:21:57,914. Validation1 RMSE on 3 timestep prediction 0.045039
2017-05-25 20:21:57,914. Validation1 RMSE on 4 timestep prediction 0.052089
2017-05-25 20:21:57,914. Validation1 RMSE on 5 timestep prediction 0.059464
2017-05-25 20:21:57,914.  Validation1 RMSE Naive One Timestep Shift 0.020273
2017-05-25 20:22:08,878. Validation2 RMSE on 1 timestep prediction 0.063739
2017-05-25 20:22:08,879. Validation2 RMSE on 2 timestep prediction 0.079089
2017-05-25 20:22:08,879. Validation2 RMSE on 3 timestep prediction 0.092020
2017-05-25 20:22:08,879. Validation2 RMSE on 4 timestep prediction 0.100272
2017-05-25 20:22:08,879. Validation2 RMSE on 5 timestep prediction 0.105253
2017-05-25 20:22:08,879.  Validation2 RMSE Naive One Timestep Shift 0.034259
2017-05-25 20:22:29,030. Test RMSE on 1 timestep prediction 0.063688
2017-05-25 20:22:29,030. Test RMSE on 2 timestep prediction 0.079213
2017-05-25 20:22:29,030. Test RMSE on 3 timestep prediction 0.092278
2017-05-25 20:22:29,031. Test RMSE on 4 timestep prediction 0.100629
2017-05-25 20:22:29,031. Test RMSE on 5 timestep prediction 0.105678
2017-05-25 20:22:29,031.  Test RMSE Naive One Timestep Shift 0.034259
2017-05-25 20:22:34,077. -------------------------run complete----------------------------------------------
2017-05-25 20:22:34,077. 

************************************************************************************************
---------------------------------Stateless optimization May 30--------------------------------------------


un_config = { 'Xserver' : False,
               'log_file' : 'logs/run.log',
               'experiment_id' : "ecg_may29_stateless",
               #'data_folder': 'resources/data/nab/nab_machine_temperature/',
               #'data_folder': 'resources/data/discords/space_shuttle/',
               #'data_folder': 'resources/data/discords/dutch_power/',
               'data_folder': 'resources/data/discords/ECG/',
               'save_figure': True
               }

opt_config = { 'Xserver' : False,
               'log_file' : 'logs/opt.log',
               'opt_run_id': "ECG_opt_may29",
               'data_folder': 'resources/data/discords/ECG/',
               'save_figure': True,
               'model': 'default',
               'max_iter': 50,
                'initial_evals': 10
               }

multi_step_lstm_config = {  'batch_size': 1024,
                            'n_epochs': 100,
                            'dropout': 0.2,
                            'look_back': 10,
                            'look_ahead': 5,
                            #'layers':{'input': 1, 'hidden1':20, 'hidden2':5,  'output': 1},
                            #'layers':{'input': 1, 'hidden1': 200, 'hidden2': 80, 'hidden3': 40, 'hidden4': 10,'output': 1},
                            'layers': {'input': 1, 'hidden1': 60, 'hidden2': 60, 'output': 1},
                            'loss': 'mse',
                            #'optimizer': 'adam',
                            'train_test_ratio' : 0.7,
                            'shuffle': False,
                            'validation': True,
                            'learning_rate': .1,
                            'patience': 2,
                           }

2017-05-30 09:45:34,126. ==Optimization Hyperparams List==
2017-05-30 09:45:34,126. [[  1.09745773e-01   7.63771801e-02   5.12000000e+02   2.00000000e+00
    1.00000000e+00]
 [  3.01447751e-01   2.57913306e-02   1.28000000e+02   6.00000000e+00
    2.00000000e+00]
 [  2.44334649e-01   1.19785859e-02   1.02400000e+03   2.00000000e+01
    1.00000000e+00]
 [  2.39278658e-01   3.73398212e-02   1.02400000e+03   2.00000000e+01
    0.00000000e+00]
 [  1.14821842e-01   8.11404023e-02   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  4.31505627e-02   3.77070758e-02   2.56000000e+02   3.00000000e+00
    0.00000000e+00]
 [  1.26570173e-01   6.15583020e-02   1.02400000e+03   1.00000000e+00
    0.00000000e+00]
 [  2.12043287e-01   7.44018709e-02   1.02400000e+03   1.00000000e+00
    0.00000000e+00]
 [  1.61857762e-01   5.65339567e-02   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  2.25400752e-02   7.13141063e-02   2.56000000e+02   6.00000000e+00
    0.00000000e+00]
 [  1.14821820e-01   8.11402412e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  4.00000000e-01   1.00000000e-02   5.12000000e+02   1.00000000e+01
    0.00000000e+00]
 [  4.00000000e-01   1.00000000e-02   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  0.00000000e+00   1.00000000e-01   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  1.62377011e-01   5.75561092e-02   2.56000000e+02   1.00000000e+01
    1.00000000e+00]
 [  1.62396757e-01   5.75922881e-02   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  1.09745831e-01   7.63771672e-02   5.12000000e+02   1.00000000e+00
    1.00000000e+00]
 [  1.09745515e-01   7.63772426e-02   5.12000000e+02   3.00000000e+00
    1.00000000e+00]
 [  1.09745466e-01   7.63772565e-02   5.12000000e+02   3.00000000e+00
    2.00000000e+00]
 [  1.09745663e-01   7.63772094e-02   5.12000000e+02   2.00000000e+00
    2.00000000e+00]
 [  1.09745386e-01   7.63772813e-02   5.12000000e+02   3.00000000e+00
    0.00000000e+00]
 [  1.64972219e-01   9.04335372e-02   5.12000000e+02   2.00000000e+00
    0.00000000e+00]
 [  2.24098911e-02   7.19255861e-02   2.56000000e+02   6.00000000e+00
    1.00000000e+00]
 [  4.00000000e-01   1.00000000e-02   2.56000000e+02   6.00000000e+00
    1.00000000e+00]
 [  3.21345842e-01   9.51963625e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  4.00000000e-01   1.00000000e-01   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  4.00000000e-01   1.00000000e-01   2.56000000e+02   1.00000000e+01
    1.00000000e+00]
 [  4.00000000e-01   1.00000000e-02   5.12000000e+02   2.00000000e+00
    1.00000000e+00]
 [  2.25744757e-01   1.00000000e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  3.76015035e-02   3.49976599e-02   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  1.68700462e-02   1.00000000e-01   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  4.00000000e-01   1.00000000e-01   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  0.00000000e+00   1.00000000e-01   5.12000000e+02   2.00000000e+00
    1.00000000e+00]
 [  3.83845384e-01   9.91659508e-02   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  0.00000000e+00   1.00000000e-01   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  3.75879183e-01   8.01039941e-02   5.12000000e+02   3.00000000e+00
    2.00000000e+00]
 [  3.78472008e-01   9.74085380e-02   5.12000000e+02   3.00000000e+00
    1.00000000e+00]
 [  0.00000000e+00   1.00000000e-01   2.56000000e+02   1.00000000e+01
    1.00000000e+00]
 [  0.00000000e+00   1.00000000e-02   2.56000000e+02   1.00000000e+01
    1.00000000e+00]
 [  2.79967087e-01   9.63239360e-02   2.56000000e+02   6.00000000e+00
    0.00000000e+00]
 [  3.96063008e-01   8.15541700e-02   5.12000000e+02   2.00000000e+00
    2.00000000e+00]
 [  3.02120445e-01   1.00000000e-02   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  5.93369777e-02   9.29830792e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  5.52551232e-02   1.00000000e-01   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  2.45873083e-02   3.94219466e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  7.48221477e-02   1.00000000e-01   2.56000000e+02   1.00000000e+01
    1.00000000e+00]
 [  8.94165706e-02   8.91077134e-02   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  6.18945287e-02   2.21644251e-02   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  6.59300440e-02   1.23285849e-02   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  1.32047404e-01   1.00000000e-01   2.56000000e+02   1.00000000e+01
    0.00000000e+00]
 [  2.44253459e-02   2.52273807e-02   5.12000000e+02   2.00000000e+00
    1.00000000e+00]
 [  9.30105791e-02   1.00000000e-01   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  0.00000000e+00   1.00000000e-02   2.56000000e+02   6.00000000e+00
    1.00000000e+00]
 [  3.22227694e-02   9.90391868e-02   2.56000000e+02   6.00000000e+00
    1.00000000e+00]
 [  7.53764629e-02   5.56159981e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  6.48335827e-02   8.65708012e-02   2.56000000e+02   6.00000000e+00
    1.00000000e+00]
 [  3.65162556e-01   5.86780075e-02   5.12000000e+02   1.00000000e+01
    1.00000000e+00]
 [  1.26680210e-01   2.07937290e-02   5.12000000e+02   1.00000000e+01
    2.00000000e+00]
 [  2.15737314e-01   1.00000000e-01   2.56000000e+02   1.00000000e+01
    2.00000000e+00]
 [  1.74110082e-01   1.00000000e-01   5.12000000e+02   1.00000000e+01
    2.00000000e+00]]
2017-05-30 09:45:34,132. ==Optimum Hyperparams==
2017-05-30 09:45:34,132. [  3.22227694e-02   9.90391868e-02   2.56000000e+02   6.00000000e+00
   1.00000000e+00]
2017-05-30 09:45:34,132. Min validation loss 0.083345
2017-05-30 09:45:34,156. =================End Optimizing=====================

layers_array = [{'input': 1, 'hidden1': 40, 'output': 1},
                    {'input': 1, 'hidden1': 60, 'hidden2': 30, 'output': 1},
                    {'input': 1, 'hidden1': 60, 'hidden2': 60,  'output': 1}


multistep_domain =[{'name': 'dropout', 'type': 'continuous', 'domain': (0.0,0.4)},
               {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.01,.1)},
               {'name': 'batch_size', 'type': 'discrete', 'domain': (128,256,512,1024)},
               {'name': 'look_back', 'type': 'discrete', 'domain': (1,2,3,6,10,15,20)},
               {'name': 'layers', 'type':'discrete', 'domain':(0,1,2)}


#######################################################################################################################
/usr/bin/python2.7 /home/akash/projects/thesis/lstm_anomaly_thesis/lstm_predictor.py
Using TensorFlow backend.
Loading data...
data mean -0.000000, data variance 1.000000
/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)
/home/akash/projects/thesis/lstm_anomaly_thesis/models/lstm.py:104: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.
  return_sequences= True if self.n_hidden>1 else False))
/home/akash/projects/thesis/lstm_anomaly_thesis/models/lstm.py:104: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=60, return_sequences=True, input_shape=(8, 1))`
  return_sequences= True if self.n_hidden>1 else False))
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
/home/akash/projects/thesis/lstm_anomaly_thesis/models/lstm.py:116: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`
  self.model.add(Dense(output_dim=self.layers['output']))
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lstm_1 (LSTM)                (None, 8, 60)             14880
_________________________________________________________________
dropout_1 (Dropout)          (None, 8, 60)             0
_________________________________________________________________
lstm_2 (LSTM)                (None, 30)                10920
_________________________________________________________________
dropout_2 (Dropout)          (None, 30)                0
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 31
_________________________________________________________________
repeat_vector_1 (RepeatVecto (None, 5, 1)              0
_________________________________________________________________
activation_1 (Activation)    (None, 5, 1)              0
=================================================================
Total params: 25,831
Trainable params: 25,831
Non-trainable params: 0
_________________________________________________________________
Train on 11947 samples, validate on 1307 samples
Epoch 1/100
3s - loss: 0.9065 - val_loss: 0.3158
Epoch 2/100
3s - loss: 0.1990 - val_loss: 0.1745
Epoch 3/100
2s - loss: 0.1638 - val_loss: 0.1592
Epoch 4/100
1s - loss: 0.1547 - val_loss: 0.1534
Epoch 5/100
2s - loss: 0.1483 - val_loss: 0.1463
Epoch 6/100
1s - loss: 0.1457 - val_loss: 0.1432
Epoch 7/100
1s - loss: 0.1424 - val_loss: 0.1396
Epoch 8/100
1s - loss: 0.1409 - val_loss: 0.1378
Epoch 9/100
1s - loss: 0.1413 - val_loss: 0.1360
Epoch 10/100
1s - loss: 0.1379 - val_loss: 0.1340
Epoch 11/100
1s - loss: 0.1363 - val_loss: 0.1327
Epoch 12/100
1s - loss: 0.1376 - val_loss: 0.1311
Epoch 13/100
1s - loss: 0.1336 - val_loss: 0.1295
Epoch 14/100
1s - loss: 0.1325 - val_loss: 0.1282
Epoch 15/100
1s - loss: 0.1294 - val_loss: 0.1283
Epoch 16/100
1s - loss: 0.1300 - val_loss: 0.1264
Epoch 17/100
1s - loss: 0.1284 - val_loss: 0.1256
Epoch 18/100
1s - loss: 0.1278 - val_loss: 0.1248
Epoch 19/100
1s - loss: 0.1264 - val_loss: 0.1236
Epoch 20/100
1s - loss: 0.1263 - val_loss: 0.1228
Epoch 21/100
1s - loss: 0.1258 - val_loss: 0.1225
Epoch 22/100
1s - loss: 0.1245 - val_loss: 0.1211
Epoch 23/100
1s - loss: 0.1234 - val_loss: 0.1205
Epoch 24/100
1s - loss: 0.1253 - val_loss: 0.1197
Epoch 25/100
1s - loss: 0.1227 - val_loss: 0.1187
Epoch 26/100
1s - loss: 0.1239 - val_loss: 0.1186
Epoch 27/100
1s - loss: 0.1222 - val_loss: 0.1185
Epoch 28/100
1s - loss: 0.1194 - val_loss: 0.1174
Epoch 29/100
1s - loss: 0.1213 - val_loss: 0.1169
Epoch 30/100
1s - loss: 0.1218 - val_loss: 0.1163
Epoch 31/100
1s - loss: 0.1194 - val_loss: 0.1158
Epoch 32/100
1s - loss: 0.1198 - val_loss: 0.1155
Epoch 33/100
1s - loss: 0.1191 - val_loss: 0.1145
Epoch 34/100
1s - loss: 0.1173 - val_loss: 0.1137
Epoch 35/100
2s - loss: 0.1193 - val_loss: 0.1135
Epoch 36/100
2s - loss: 0.1197 - val_loss: 0.1131
Epoch 37/100
1s - loss: 0.1175 - val_loss: 0.1127
Epoch 38/100
1s - loss: 0.1181 - val_loss: 0.1124
Epoch 39/100
1s - loss: 0.1163 - val_loss: 0.1120
Epoch 40/100
1s - loss: 0.1166 - val_loss: 0.1115
Epoch 41/100
1s - loss: 0.1178 - val_loss: 0.1115
Epoch 42/100
1s - loss: 0.1165 - val_loss: 0.1105
Epoch 43/100
1s - loss: 0.1152 - val_loss: 0.1104
Epoch 44/100
1s - loss: 0.1173 - val_loss: 0.1102
Epoch 45/100
1s - loss: 0.1145 - val_loss: 0.1093
Epoch 46/100
1s - loss: 0.1142 - val_loss: 0.1090
Epoch 47/100
1s - loss: 0.1126 - val_loss: 0.1084
Epoch 48/100
1s - loss: 0.1123 - val_loss: 0.1080
Epoch 49/100
1s - loss: 0.1121 - val_loss: 0.1078
Epoch 50/100
1s - loss: 0.1118 - val_loss: 0.1074
Epoch 51/100
1s - loss: 0.1118 - val_loss: 0.1072
Epoch 52/100
1s - loss: 0.1121 - val_loss: 0.1069
Epoch 53/100
1s - loss: 0.1129 - val_loss: 0.1063
Epoch 54/100
1s - loss: 0.1102 - val_loss: 0.1063
Epoch 55/100
2s - loss: 0.1109 - val_loss: 0.1061
Epoch 56/100
3s - loss: 0.1111 - val_loss: 0.1056
Epoch 57/100
2s - loss: 0.1102 - val_loss: 0.1054
Epoch 58/100
1s - loss: 0.1087 - val_loss: 0.1055
Epoch 59/100
2s - loss: 0.1100 - val_loss: 0.1050
Epoch 60/100
2s - loss: 0.1098 - val_loss: 0.1047
Epoch 61/100
1s - loss: 0.1117 - val_loss: 0.1046
Epoch 62/100
1s - loss: 0.1112 - val_loss: 0.1044
Epoch 63/100
1s - loss: 0.1104 - val_loss: 0.1039
Epoch 64/100
2s - loss: 0.1088 - val_loss: 0.1033
Epoch 65/100
2s - loss: 0.1062 - val_loss: 0.1030
Epoch 66/100
1s - loss: 0.1098 - val_loss: 0.1031
Epoch 67/100
1s - loss: 0.1075 - val_loss: 0.1027
Epoch 68/100
1s - loss: 0.1078 - val_loss: 0.1024
Epoch 69/100
1s - loss: 0.1070 - val_loss: 0.1022
Epoch 70/100
1s - loss: 0.1058 - val_loss: 0.1023
Epoch 71/100
1s - loss: 0.1063 - val_loss: 0.1020
Epoch 72/100
1s - loss: 0.1077 - val_loss: 0.1015
Epoch 73/100
1s - loss: 0.1065 - val_loss: 0.1016
Epoch 74/100
1s - loss: 0.1082 - val_loss: 0.1013
Epoch 75/100
2s - loss: 0.1090 - val_loss: 0.1011
Epoch 76/100
2s - loss: 0.1077 - val_loss: 0.1011
Epoch 77/100
1s - loss: 0.1085 - val_loss: 0.1010
Epoch 78/100
1s - loss: 0.1060 - val_loss: 0.1006
Epoch 79/100
1s - loss: 0.1059 - val_loss: 0.1005
Epoch 80/100
1s - loss: 0.1057 - val_loss: 0.1002
Epoch 81/100
1s - loss: 0.1052 - val_loss: 0.1000
Epoch 82/100
1s - loss: 0.1061 - val_loss: 0.1001
Epoch 83/100
1s - loss: 0.1058 - val_loss: 0.1000
['loss', 'val_loss']
Validation2 Loss 0.346013013849
Test Loss 0.346013013849
(11947, 5, 1)
(1307, 5, 1)
Calculated validation1 loss 0.199290
(4737, 5, 1)
Calculated validation2 loss 0.433001
(4737, 5, 1)
Calculated test loss 0.433001

Process finished with exit code 0

multi_step_lstm_config = {  'batch_size': 256,
                            'n_epochs': 100,
                            'dropout': 0.2 ,
                            'look_back': 8,
                            'look_ahead': 5,
                            #'layers':{'input': 1, 'hidden1':20, 'hidden2':5,  'output': 1},
                            #'layers':{'input': 1, 'hidden1': 200, 'hidden2': 80, 'hidden3': 40, 'hidden4': 10,'output': 1},
                            'layers': {'input': 1, 'hidden1': 60, 'hidden2': 30, 'output': 1},
                            'loss': 'mse',
                            #'optimizer': 'adam',
                            'train_test_ratio' : 0.7,
                            'shuffle': True,
                            'validation': True,
                            'learning_rate': .1,
                            'patience': 1,
                           }
#